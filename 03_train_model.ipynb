{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f3246f",
   "metadata": {},
   "source": [
    "# 03 ‚Äî Train Sentiment Model (distilbert-multilingual)\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "- Fine-tune `distilbert-base-multilingual-cased` tr√™n EN train set\n",
    "- Optimize cho GTX 1650: batch nh·ªè (4-8), fp16, max_length=128, early stopping\n",
    "- L∆∞u model ƒë·ªÉ d√πng cho evaluation + XAI\n",
    "\n",
    "**L√Ω do ch·ªçn distilbert-multilingual:**\n",
    "- Nh·∫π h∆°n XLM-R, BERT-multilingual\n",
    "- V·∫´n support multilingual (d√π train EN, test ES/FR v·∫´n work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4193474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcfc23",
   "metadata": {},
   "source": [
    "## 1) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba066364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path('data_splits')\n",
    "OUTPUT_DIR = Path('model_output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Columns (MUST match 02_sample_split.ipynb)\n",
    "TEXT_COL = 'cleaned_text'  # actual column name from CSV\n",
    "LABEL_COL = 'sentiment'\n",
    "\n",
    "# Model - BERT base multilingual (178M params, larger than distilbert 135M)\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'  # or 'xlm-roberta-base' (270M) if you have more VRAM\n",
    "\n",
    "# Training hyperparameters (optimized for GTX 1650)\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 8  # reduced from 16 due to larger model (increase gradient_accumulation if needed)\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 5  # 5 epochs for better convergence, early stopping will handle convergence\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "FP16 = torch.cuda.is_available()  # use mixed precision if GPU\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOP_PATIENCE = 2\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f3e03",
   "metadata": {},
   "source": [
    "## 2) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6f4d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (31499, 2)\n",
      "Val shape: (6750, 2)\n",
      "\n",
      "üìã Columns in train.csv:\n",
      "['cleaned_text', 'sentiment']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brute Force took down our server.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So into pressure enjoy single box check knowle...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text sentiment\n",
       "0                  Brute Force took down our server.  negative\n",
       "1  So into pressure enjoy single box check knowle...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution (train):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    10500\n",
       "neutral     10500\n",
       "positive    10499\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "df_val = pd.read_csv(DATA_DIR / 'val.csv')\n",
    "\n",
    "print('Train shape:', df_train.shape)\n",
    "print('Val shape:', df_val.shape)\n",
    "\n",
    "# Check actual columns in the data\n",
    "print('\\nüìã Columns in train.csv:')\n",
    "print(df_train.columns.tolist())\n",
    "\n",
    "print('\\nFirst few rows:')\n",
    "display(df_train.head(2))\n",
    "\n",
    "print('\\nLabel distribution (train):')\n",
    "display(df_train[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d3a03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset: 10000 train, 2000 val\n",
      "Sampled train: 9999\n",
      "Sampled val: 1998\n",
      "\n",
      "Final shapes:\n",
      "Train: (9999, 2)\n",
      "Val: (1998, 2)\n",
      "\n",
      "Label distribution (train):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nguyen Ngo\\AppData\\Local\\Temp\\ipykernel_7712\\2362040052.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_train = df_train.groupby(LABEL_COL, group_keys=False).apply(\n",
      "C:\\Users\\Nguyen Ngo\\AppData\\Local\\Temp\\ipykernel_7712\\2362040052.py:15: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_val = df_val.groupby(LABEL_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    3333\n",
       "neutral     3333\n",
       "positive    3333\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For faster training, use smaller subset (stratified sampling)\n",
    "TRAIN_SUBSET_SIZE = 10000  # reduce from 31k to 10k for speed\n",
    "VAL_SUBSET_SIZE = 2000     # reduce from 6.7k to 2k\n",
    "\n",
    "print(f'Using subset: {TRAIN_SUBSET_SIZE} train, {VAL_SUBSET_SIZE} val')\n",
    "\n",
    "# Stratified sample\n",
    "if len(df_train) > TRAIN_SUBSET_SIZE:\n",
    "    df_train = df_train.groupby(LABEL_COL, group_keys=False).apply(\n",
    "        lambda x: x.sample(min(TRAIN_SUBSET_SIZE // df_train[LABEL_COL].nunique(), len(x)), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    print(f'Sampled train: {len(df_train)}')\n",
    "\n",
    "if len(df_val) > VAL_SUBSET_SIZE:\n",
    "    df_val = df_val.groupby(LABEL_COL, group_keys=False).apply(\n",
    "        lambda x: x.sample(min(VAL_SUBSET_SIZE // df_val[LABEL_COL].nunique(), len(x)), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    print(f'Sampled val: {len(df_val)}')\n",
    "\n",
    "print('\\nFinal shapes:')\n",
    "print('Train:', df_train.shape)\n",
    "print('Val:', df_val.shape)\n",
    "print('\\nLabel distribution (train):')\n",
    "display(df_train[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf52aee",
   "metadata": {},
   "source": [
    "## 3) Encode labels to integers (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e2e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "NUM_LABELS: 3\n"
     ]
    }
   ],
   "source": [
    "# Check if labels are already numeric\n",
    "if df_train[LABEL_COL].dtype in ['object', 'string']:\n",
    "    # Encode to int\n",
    "    label_map = {lbl: i for i, lbl in enumerate(sorted(df_train[LABEL_COL].unique()))}\n",
    "    print('Label mapping:', label_map)\n",
    "    \n",
    "    df_train['label_id'] = df_train[LABEL_COL].map(label_map)\n",
    "    df_val['label_id'] = df_val[LABEL_COL].map(label_map)\n",
    "    \n",
    "    LABEL_ID_COL = 'label_id'\n",
    "    NUM_LABELS = len(label_map)\n",
    "    \n",
    "    # Save label map for later\n",
    "    import json\n",
    "    with open(OUTPUT_DIR / 'label_map.json', 'w') as f:\n",
    "        json.dump(label_map, f, indent=2)\n",
    "else:\n",
    "    # Already numeric\n",
    "    LABEL_ID_COL = LABEL_COL\n",
    "    NUM_LABELS = df_train[LABEL_COL].nunique()\n",
    "    print('Labels already numeric, num_labels:', NUM_LABELS)\n",
    "\n",
    "print(f'NUM_LABELS: {NUM_LABELS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb2511",
   "metadata": {},
   "source": [
    "## 4) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66f375b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nguyen Ngo\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9999/9999 [00:00<00:00, 11473.20 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1998/1998 [00:00<00:00, 14529.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 9999\n",
      "})\n",
      "Val dataset: Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1998\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_ds = Dataset.from_pandas(df_train[[TEXT_COL, LABEL_ID_COL]].rename(columns={TEXT_COL: 'text', LABEL_ID_COL: 'label'}))\n",
    "val_ds = Dataset.from_pandas(df_val[[TEXT_COL, LABEL_ID_COL]].rename(columns={TEXT_COL: 'text', LABEL_ID_COL: 'label'}))\n",
    "\n",
    "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "val_ds = val_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Set format\n",
    "train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print('Train dataset:', train_ds)\n",
    "print('Val dataset:', val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219aa8b",
   "metadata": {},
   "source": [
    "## 5) Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e1fe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 177.9M\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "print(f'Model params: {model.num_parameters() / 1e6:.1f}M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a4e71",
   "metadata": {},
   "source": [
    "## 6) Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9407bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc76d4e",
   "metadata": {},
   "source": [
    "## 7) Training arguments (optimized for GTX 1650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "665a34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=300,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=model_output\\logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=50,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=f1_macro,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=model_output,\n",
      "overwrite_output_dir=False,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=300,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    \n",
    "    # Training params\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,  # eval can use larger batch\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=FP16,\n",
    "    gradient_accumulation_steps=2,  # increased from 1 to simulate batch 16 with less VRAM usage\n",
    "    \n",
    "    # Evaluation & saving (less frequent to speed up)\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=300,  # evaluate every 300 steps\n",
    "    save_strategy='steps',\n",
    "    save_steps=300,\n",
    "    save_total_limit=2,  # keep only 2 best checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "    \n",
    "    # Misc\n",
    "    seed=RANDOM_STATE,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "print('Training args:', training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed45bf",
   "metadata": {},
   "source": [
    "## 8) Trainer + early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a71129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nguyen Ngo\\AppData\\Local\\Temp\\ipykernel_7712\\1694619926.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready. Starting training...\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)]\n",
    ")\n",
    "\n",
    "print('Trainer ready. Starting training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99f007",
   "metadata": {},
   "source": [
    "## 9) Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf84d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 3:24:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>0.669696</td>\n",
       "      <td>0.721722</td>\n",
       "      <td>0.708132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.382255</td>\n",
       "      <td>0.869870</td>\n",
       "      <td>0.868247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.269228</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>0.912519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.222109</td>\n",
       "      <td>0.921421</td>\n",
       "      <td>0.920468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.290651</td>\n",
       "      <td>0.929429</td>\n",
       "      <td>0.928304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.938438</td>\n",
       "      <td>0.937974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.244401</td>\n",
       "      <td>0.939940</td>\n",
       "      <td>0.939708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.265875</td>\n",
       "      <td>0.946446</td>\n",
       "      <td>0.946186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.274664</td>\n",
       "      <td>0.946446</td>\n",
       "      <td>0.946157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.264754</td>\n",
       "      <td>0.950951</td>\n",
       "      <td>0.950585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training complete!\n",
      "Best metric (f1_macro): N/A\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print('\\n‚úÖ Training complete!')\n",
    "print('Best metric (f1_macro):', train_result.metrics.get('eval_f1_macro', 'N/A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99dad99",
   "metadata": {},
   "source": [
    "## 10) Evaluate on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b37b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation results:\n",
      "  eval_loss: 0.2648\n",
      "  eval_accuracy: 0.9510\n",
      "  eval_f1_macro: 0.9506\n",
      "  eval_runtime: 133.2799\n",
      "  eval_samples_per_second: 14.9910\n",
      "  eval_steps_per_second: 0.9380\n",
      "  epoch: 5.0000\n"
     ]
    }
   ],
   "source": [
    "val_results = trainer.evaluate()\n",
    "\n",
    "print('\\nValidation results:')\n",
    "for k, v in val_results.items():\n",
    "    print(f'  {k}: {v:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a6351",
   "metadata": {},
   "source": [
    "## 11) Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce5c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model saved to C:\\Paper\\Twitter proejct\\Twitter Sentiment Analysis Dataset\\Twitter Sentiment Analysis Dataset\\model_output\\final_model\n"
     ]
    }
   ],
   "source": [
    "# Save model + tokenizer\n",
    "FINAL_MODEL_DIR = OUTPUT_DIR / 'final_model'\n",
    "trainer.save_model(str(FINAL_MODEL_DIR))\n",
    "tokenizer.save_pretrained(str(FINAL_MODEL_DIR))\n",
    "\n",
    "print(f'\\n‚úÖ Model saved to {FINAL_MODEL_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4605d",
   "metadata": {},
   "source": [
    "## 12) Test on val set with classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf4568a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(val_ds)\n\u001b[0;32m      3\u001b[0m val_pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(val_preds\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m val_true_labels \u001b[38;5;241m=\u001b[39m val_preds\u001b[38;5;241m.\u001b[39mlabel_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "val_preds = trainer.predict(val_ds)\n",
    "val_pred_labels = np.argmax(val_preds.predictions, axis=-1)\n",
    "val_true_labels = val_preds.label_ids\n",
    "\n",
    "# Classification report\n",
    "print('\\nClassification report (validation set):')\n",
    "print(classification_report(val_true_labels, val_pred_labels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fe7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "- Notebook 04: Translate val/test EN‚ÜíES/FR + evaluate robustness\n",
    "- Notebook 05: XAI + explanation consistency metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
